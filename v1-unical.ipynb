{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-15T19:31:31.429316Z",
     "iopub.status.busy": "2025-01-15T19:31:31.428934Z",
     "iopub.status.idle": "2025-01-15T19:31:31.436378Z",
     "shell.execute_reply": "2025-01-15T19:31:31.435340Z",
     "shell.execute_reply.started": "2025-01-15T19:31:31.429289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/miniconda3/envs/xray-captioning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    ResNetForImageClassification,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T19:31:31.438371Z",
     "iopub.status.busy": "2025-01-15T19:31:31.438019Z",
     "iopub.status.idle": "2025-01-15T19:31:35.061256Z",
     "shell.execute_reply": "2025-01-15T19:31:35.060131Z",
     "shell.execute_reply.started": "2025-01-15T19:31:31.438345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Kaggle using kagglehub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_773356/3071052731.py:8: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
      "  df_image = kagglehub.load_dataset(\n",
      "/tmp/ipykernel_773356/3071052731.py:15: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
      "  df_report = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Images dataset shape: (7466, 3)\n",
      "Reports dataset shape: (3851, 8)\n",
      "Final dataset with valid captions: 6469 samples\n",
      "Dataset downloaded to: /home/alexandre/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2\n",
      "Checking if image files exist:\n",
      "  /home/alexandre/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2/images/images_normalized/1_IM-0001-4001.dcm.png: ✓\n",
      "  /home/alexandre/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2/images/images_normalized/1_IM-0001-3001.dcm.png: ✓\n",
      "  /home/alexandre/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2/images/images_normalized/2_IM-0652-1001.dcm.png: ✓\n",
      "Dataset object created with 6469 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "# Load the CSV files using kagglehub\n",
    "print(\"Loading dataset from Kaggle using kagglehub...\")\n",
    "\n",
    "# Load the projections CSV\n",
    "df_image = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"raddar/chest-xrays-indiana-university\",\n",
    "    \"indiana_projections.csv\"\n",
    ")\n",
    "\n",
    "# Load the reports CSV\n",
    "df_report = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"raddar/chest-xrays-indiana-university\", \n",
    "    \"indiana_reports.csv\"\n",
    ")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Images dataset shape: {df_image.shape}\")\n",
    "print(f\"Reports dataset shape: {df_report.shape}\")\n",
    "\n",
    "# Create a DataFrame for images and captions\n",
    "data = []\n",
    "for i in range(len(df_image)):\n",
    "    uid = df_image.iloc[i]['uid']\n",
    "    image = df_image.iloc[i]['filename']\n",
    "    index = df_report.loc[df_report['uid'] == uid]\n",
    "    \n",
    "    if not index.empty:    \n",
    "        index = index.index[0]\n",
    "        caption = df_report.iloc[index]['findings']\n",
    "        if isinstance(caption, float):  # Skip rows with missing captions\n",
    "            continue\n",
    "        data.append({'imgs': image, 'captions': caption})\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Final dataset with valid captions: {len(df)} samples\")\n",
    "\n",
    "# Download the dataset files to get the images path\n",
    "dataset_path = kagglehub.dataset_download(\"raddar/chest-xrays-indiana-university\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Update image paths to use the downloaded dataset path\n",
    "images_path = os.path.join(dataset_path, \"images\", \"images_normalized\")\n",
    "df['imgs'] = df['imgs'].apply(lambda x: os.path.join(images_path, x))\n",
    "\n",
    "# Verify first few image paths exist\n",
    "print(\"Checking if image files exist:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    img_path = df.iloc[i]['imgs']\n",
    "    exists = os.path.exists(img_path)\n",
    "    print(f\"  {img_path}: {'✓' if exists else '✗'}\")\n",
    "\n",
    "# Convert pandas DataFrame to a Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(f\"Dataset object created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T19:31:35.063347Z",
     "iopub.status.busy": "2025-01-15T19:31:35.063083Z",
     "iopub.status.idle": "2025-01-15T19:31:35.928685Z",
     "shell.execute_reply": "2025-01-15T19:31:35.927517Z",
     "shell.execute_reply.started": "2025-01-15T19:31:35.063320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load ResNet-50 for feature extraction (frozen)\n",
    "resnet_model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\").to(device)\n",
    "resnet_model.eval()  # We won't train the ResNet, just use it for feature extraction\n",
    "\n",
    "# Load GPT-2 for language generation\n",
    "gpt2_model_name = \"gpt2\"  # or \"distilgpt2\" for a lighter version\n",
    "tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "# GPT-2 doesn't have a pad token by default, let's assign one:\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T19:31:35.931013Z",
     "iopub.status.busy": "2025-01-15T19:31:35.930296Z",
     "iopub.status.idle": "2025-01-15T19:31:36.196941Z",
     "shell.execute_reply": "2025-01-15T19:31:36.196049Z",
     "shell.execute_reply.started": "2025-01-15T19:31:35.930962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `use_fast=True` but `torchvision` is not available. Falling back to the slow image processor.\n"
     ]
    }
   ],
   "source": [
    "# Processor for ResNet images\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\", use_fast=True)\n",
    "def preprocess_images_and_captions(example):\n",
    "    # Process the image\n",
    "    image = Image.open(example[\"imgs\"]).convert(\"L\")  # Convert to grayscale\n",
    "    image = Image.merge(\"RGB\", [image, image, image])  # Convert grayscale to RGB\n",
    "    image_inputs = image_processor(image, return_tensors=\"pt\")\n",
    "    pixel_values = image_inputs[\"pixel_values\"].squeeze(0)  # Shape [3, 224, 224]\n",
    "\n",
    "    # Tokenize the caption\n",
    "    text_inputs = tokenizer(\n",
    "        example[\"captions\"],\n",
    "        truncation=True,\n",
    "        max_length=32, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values.tolist(),  # Convert tensor to list\n",
    "        \"input_ids\": text_inputs[\"input_ids\"].squeeze(0).tolist(),  # Convert tensor to list\n",
    "        \"attention_mask\": text_inputs[\"attention_mask\"].squeeze(0).tolist(),  # Convert tensor to list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset in smaller chunks to avoid memory overflow\n",
    "def process_dataset_in_chunks(dataset, chunk_size=1000,num_workers=5):\n",
    "    total_samples = len(dataset)\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for i in range(0, total_samples, chunk_size):\n",
    "        end_idx = min(i + chunk_size, total_samples)\n",
    "        chunk = dataset.select(range(i, end_idx))\n",
    "        \n",
    "        print(f\"Processing chunk {i//chunk_size + 1}/{(total_samples-1)//chunk_size + 1}\")\n",
    "        \n",
    "        processed_chunk = chunk.map(\n",
    "            preprocess_images_and_captions,\n",
    "            num_proc=num_workers,  \n",
    "            batched=False,\n",
    "            desc=f\"Chunk {i//chunk_size + 1}\"\n",
    "        )\n",
    "        \n",
    "        processed_chunks.append(processed_chunk)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    from datasets import concatenate_datasets\n",
    "    return concatenate_datasets(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T19:31:36.198343Z",
     "iopub.status.busy": "2025-01-15T19:31:36.197966Z",
     "iopub.status.idle": "2025-01-15T19:50:24.582641Z",
     "shell.execute_reply": "2025-01-15T19:50:24.581184Z",
     "shell.execute_reply.started": "2025-01-15T19:31:36.198308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training dataset with multiple workers...\n",
      "Processing training dataset in chunks...\n",
      "Processing chunk 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1 (num_proc=10): 100%|██████████| 1500/1500 [00:22<00:00, 66.89 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 2 (num_proc=10): 100%|██████████| 1500/1500 [00:23<00:00, 63.84 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 3 (num_proc=10): 100%|██████████| 1500/1500 [00:23<00:00, 63.24 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 4 (num_proc=10): 100%|██████████| 1322/1322 [00:21<00:00, 61.11 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing dataset in chunks...\n",
      "Processing chunk 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1 (num_proc=10): 100%|██████████| 647/647 [00:10<00:00, 62.74 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "num_workers = 10  # Adjust based on your CPU cores\n",
    "batch_size = 50  # Process in batches for better efficiency\n",
    "\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)  # 60% train, 40% test\n",
    "\n",
    "training_dataset = split['train']\n",
    "testing_dataset = split['test']\n",
    "# clean up the memory\n",
    "del dataset, split\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Preprocessing training dataset with multiple workers...\")\n",
    "print(\"Processing training dataset in chunks...\")\n",
    "training_dataset = process_dataset_in_chunks(training_dataset, chunk_size=1500, num_workers=num_workers)\n",
    "\n",
    "print(\"Processing testing dataset in chunks...\")\n",
    "testing_dataset = process_dataset_in_chunks(testing_dataset, chunk_size=1500, num_workers=num_workers)\n",
    "\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T19:50:24.584594Z",
     "iopub.status.busy": "2025-01-15T19:50:24.584332Z",
     "iopub.status.idle": "2025-01-15T19:50:24.644619Z",
     "shell.execute_reply": "2025-01-15T19:50:24.643389Z",
     "shell.execute_reply.started": "2025-01-15T19:50:24.584571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "def combined_collate_fn(batch):\n",
    "    pixel_values_list = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "        # Validate and convert pixel values\n",
    "        pv = torch.tensor(item[\"pixel_values\"], dtype=torch.float32)\n",
    "        if pv.shape != torch.Size([3, 224, 224]):\n",
    "            raise ValueError(f\"Expected pixel_values shape [3, 224, 224], got {pv.shape}\")\n",
    "        pixel_values_list.append(pv)\n",
    "        \n",
    "        # Convert text data\n",
    "        input_ids_list.append(torch.tensor(item[\"input_ids\"], dtype=torch.long))\n",
    "        attention_mask_list.append(torch.tensor(item[\"attention_mask\"], dtype=torch.long))\n",
    "    \n",
    "    # Stack pixel values into a single tensor\n",
    "    pixel_values = torch.stack(pixel_values_list, dim=0)  # [batch_size, 3, 224, 224]\n",
    "    \n",
    "    # Use Hugging Face DataCollatorWithPadding for tokenized text\n",
    "    text_batch = {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "    }\n",
    "    text_batch = text_collator(text_batch)\n",
    "    \n",
    "    # Add pixel values to the text batch\n",
    "    text_batch[\"pixel_values\"] = pixel_values\n",
    "    \n",
    "    return text_batch\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=10,         # only 2 samples in this example\n",
    "    shuffle=True,\n",
    "    collate_fn=combined_collate_fn,\n",
    "    drop_last=True,      # can be True if you have many samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T19:50:24.646162Z",
     "iopub.status.busy": "2025-01-15T19:50:24.645756Z",
     "iopub.status.idle": "2025-01-15T19:50:24.662985Z",
     "shell.execute_reply": "2025-01-15T19:50:24.661937Z",
     "shell.execute_reply.started": "2025-01-15T19:50:24.646120Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeatureToCaption(nn.Module):\n",
    "    \"\"\"\n",
    "    We:\n",
    "      - Extract features from ResNet (outside this class, in the training loop, frozen)\n",
    "      - Project them to GPT-2 hidden dim\n",
    "      - Sum them with the GPT-2 token embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=2048, hidden_dim=768, gpt2_name=\"gpt2\"):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(feature_dim, hidden_dim)\n",
    "        \n",
    "        self.llm = GPT2LMHeadModel.from_pretrained(gpt2_name)\n",
    "        # Because GPT-2 doesn't define pad_token by default\n",
    "        self.llm.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    def forward(self, resnet_features, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        resnet_features: [batch_size, feature_dim]\n",
    "        input_ids:       [batch_size, seq_len]\n",
    "        attention_mask:  [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # 1) Project the ResNet features to GPT-2 hidden size\n",
    "        #    shape: [batch_size, hidden_dim]\n",
    "        projected = self.linear(resnet_features)\n",
    "\n",
    "        # 2) Expand them along seq_len dimension\n",
    "        #    shape: [batch_size, 1, hidden_dim] -> [batch_size, seq_len, hidden_dim]\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        projected = projected.unsqueeze(1).expand(batch_size, seq_len, -1)\n",
    "\n",
    "        # 3) GPT-2 token embeddings\n",
    "        #    shape: [batch_size, seq_len, hidden_dim]\n",
    "        token_embeds = self.llm.transformer.wte(input_ids)\n",
    "\n",
    "        # 4) Sum them (the simplest approach)\n",
    "        inputs_embeds = token_embeds + projected\n",
    "\n",
    "        # 5) Forward pass through GPT-2\n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids,  # for CrossEntropyLoss\n",
    "        )\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-15T19:50:24.664267Z",
     "iopub.status.busy": "2025-01-15T19:50:24.663936Z",
     "iopub.status.idle": "2025-01-15T20:23:46.042706Z",
     "shell.execute_reply": "2025-01-15T20:23:46.041382Z",
     "shell.execute_reply.started": "2025-01-15T19:50:24.664236Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/582 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/3: 100%|██████████| 582/582 [04:17<00:00,  2.26it/s, Loss=0.8602, Avg Loss=1.6300, LR=0.000084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed - Average Loss: 1.6300\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 582/582 [04:17<00:00,  2.26it/s, Loss=0.8094, Avg Loss=0.8543, LR=0.000030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 completed - Average Loss: 0.8543\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 582/582 [04:16<00:00,  2.27it/s, Loss=1.2525, Avg Loss=0.6581, LR=0.000000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 completed - Average Loss: 0.6581\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate our feature-to-caption model\n",
    "model = FeatureToCaption(gpt2_name=gpt2_model_name).to(device)\n",
    "\n",
    "################################################################################\n",
    "# Training Loop with Progress Bars\n",
    "################################################################################\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "epochs = 3\n",
    "warmup_ratio = 0.1\n",
    "num_training_steps = len(dataloader) * epochs\n",
    "scaler = torch.amp.GradScaler()\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(warmup_ratio * num_training_steps),  # 10% warmup\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Create progress bar for the current epoch\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # batch has \"pixel_values\", \"input_ids\", \"attention_mask\"\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)      # [batch_size, 3, 224, 224]\n",
    "        input_ids = batch[\"input_ids\"].to(device)            # [batch_size, seq_len]\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)  # [batch_size, seq_len]\n",
    "\n",
    "        # -------------------- Freeze ResNet & Extract Features -------------------\n",
    "        with torch.no_grad():\n",
    "            # 1) Embeddings\n",
    "            emb_out = resnet_model.resnet.embedder(pixel_values)\n",
    "            # 2) Encoder\n",
    "            enc_out = resnet_model.resnet.encoder(emb_out)\n",
    "            # 3) Pool & Flatten -> shape: [batch_size, 2048]\n",
    "            pooled_features = resnet_model.resnet.pooler(enc_out.last_hidden_state).flatten(1)\n",
    "        # -------------------------------------------------------------------------\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pooled_features, input_ids, attention_mask)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscale gradients BEFORE stepping\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        # Clip gradients after unscaling\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Step optimizer and scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Step learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Zero gradients for next iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with current loss and running average\n",
    "        current_avg_loss = total_loss / (batch_idx + 1)\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Avg Loss': f'{current_avg_loss:.4f}',\n",
    "            'LR': f'{lr_scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "\n",
    "    # Final epoch summary\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch}/{epochs} completed - Average Loss: {avg_loss:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T20:50:39.923119Z",
     "iopub.status.busy": "2025-01-15T20:50:39.922765Z",
     "iopub.status.idle": "2025-01-15T20:50:40.346554Z",
     "shell.execute_reply": "2025-01-15T20:50:40.345368Z",
     "shell.execute_reply.started": "2025-01-15T20:50:39.923096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = \"models\"\n",
    "model_name = f\"xray_captioning_v1_{timestamp}\"\n",
    "model_path = os.path.join(model_dir, f\"{model_name}.pt\")\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "    'loss': avg_loss,\n",
    "    'model_config': {\n",
    "        'feature_dim': 2048,\n",
    "        'hidden_dim': 768,\n",
    "        'gpt2_name': gpt2_model_name\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, model_path)\n",
    "torch.save(model, model_path)\n",
    "\n",
    "# ---- Loading ----\n",
    "#model = torch.load(model_path)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:09<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 100 test samples\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def evaluate_model_on_test_set(model, test_dataset, num_samples=100):\n",
    "    \"\"\"Properly evaluate model on test set\"\"\"\n",
    "    evalRefs = []\n",
    "    evalHyps = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Use tqdm for evaluation progress\n",
    "    test_samples = list(test_dataset.select(range(min(num_samples, len(test_dataset)))))\n",
    "    \n",
    "    for idx, sample in enumerate(tqdm(test_samples, desc=\"Evaluating\")):\n",
    "        with torch.no_grad():\n",
    "            # Convert to tensors\n",
    "            pixel_values = torch.tensor(sample[\"pixel_values\"]).unsqueeze(0).to(device)\n",
    "            input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "            attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "            # Extract ResNet features\n",
    "            emb_out = resnet_model.resnet.embedder(pixel_values)\n",
    "            enc_out = resnet_model.resnet.encoder(emb_out)\n",
    "            pooled_features = resnet_model.resnet.pooler(enc_out.last_hidden_state).flatten(1)\n",
    "        \n",
    "            # Generate caption\n",
    "            outputs = model.llm.generate(\n",
    "                inputs_embeds=(model.linear(pooled_features).unsqueeze(1) + \n",
    "                              model.llm.transformer.wte(input_ids)),\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=2,  # Use beam search for better quality\n",
    "                do_sample=False,  # Deterministic for evaluation\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Add to evaluation lists\n",
    "        evalRefs.append({\n",
    "            \"image_id\": idx,\n",
    "            \"caption\": sample[\"captions\"]\n",
    "        })\n",
    "        \n",
    "        evalHyps.append({\n",
    "            \"image_id\": idx,\n",
    "            \"caption\": generated_text\n",
    "        })\n",
    "    \n",
    "    return evalRefs, evalHyps\n",
    "\n",
    "# Run evaluation\n",
    "evalRefs, evalHyps = evaluate_model_on_test_set(model, testing_dataset, num_samples=100)\n",
    "print(f\"Evaluated on {len(evalHyps)} test samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexandre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/alexandre/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexandre/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Fix NLTK downloads - add this at the top of your evaluation cell\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "print(\"NLTK data downloaded successfully!\")\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sacrebleu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:05:53.383841Z",
     "iopub.status.busy": "2025-01-15T21:05:53.383429Z",
     "iopub.status.idle": "2025-01-15T21:06:19.940995Z",
     "shell.execute_reply": "2025-01-15T21:06:19.939955Z",
     "shell.execute_reply.started": "2025-01-15T21:05:53.383811Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 647 testing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions: 100%|██████████| 647/647 [01:07<00:00,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of references: 647\n",
      "Number of hypotheses: 647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install git+https://github.com/salaniz/pycocoevalcap.git\n",
    "evalRefs = []\n",
    "evalHyps = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_samples = len(testing_dataset)\n",
    "print(f\"Processing {total_samples} testing samples...\")\n",
    "\n",
    "\n",
    "for idx, sample in enumerate(tqdm(testing_dataset, desc=\"Generating captions\", total=total_samples)):\n",
    "    with torch.no_grad():\n",
    "        # Convert pixel_values back to tensor\n",
    "        pixel_values = torch.tensor(sample[\"pixel_values\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Convert input_ids and attention_mask to tensors\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(sample[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    \n",
    "        # ResNet Features\n",
    "        emb_out = resnet_model.resnet.embedder(pixel_values)\n",
    "        enc_out = resnet_model.resnet.encoder(emb_out)\n",
    "        pooled_features = resnet_model.resnet.pooler(enc_out.last_hidden_state).flatten(1)\n",
    "    \n",
    "        # Generate a caption\n",
    "        combined_embeds = (model.linear(pooled_features).unsqueeze(1) + model.llm.transformer.wte(input_ids))\n",
    "    \n",
    "        # Generate a caption\n",
    "        outputs = model.llm.generate(\n",
    "            inputs_embeds=combined_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=50,  # Increase this\n",
    "            min_length=10,  # Add minimum length\n",
    "            num_beams=3,    # More beams\n",
    "            no_repeat_ngram_size=2,  # Avoid repetition\n",
    "            do_sample=True,  # Add some randomness\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Convert references into the required format\n",
    "    # If sample[\"captions\"] is a single string:\n",
    "    if isinstance(sample[\"captions\"], str):\n",
    "        gt_captions = [sample[\"captions\"]]\n",
    "    else:\n",
    "        gt_captions = sample[\"captions\"]\n",
    "    \n",
    "    # Append references\n",
    "    # pycocoevalcap expects something like:\n",
    "    # {\"image_id\": <id>, \"caption\": \"some reference caption\"}\n",
    "    for ref in gt_captions:\n",
    "        evalRefs.append({\n",
    "            \"image_id\": idx,\n",
    "            \"caption\": ref\n",
    "        })\n",
    "\n",
    "    # Append hypothesis\n",
    "    evalHyps.append({\n",
    "        \"image_id\": idx,\n",
    "        \"caption\": generated_text\n",
    "    })\n",
    "print(f\"Number of references: {len(evalRefs)}\")\n",
    "print(f\"Number of hypotheses: {len(evalHyps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T21:06:21.222993Z",
     "iopub.status.busy": "2025-01-15T21:06:21.222639Z",
     "iopub.status.idle": "2025-01-15T21:06:21.318532Z",
     "shell.execute_reply": "2025-01-15T21:06:21.317299Z",
     "shell.execute_reply.started": "2025-01-15T21:06:21.222968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing enhanced evaluation metrics...\n",
      "Evaluating 567 valid pairs out of 647 total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENHANCED EVALUATION RESULTS\n",
      "============================================================\n",
      "SIMILARITY METRICS:\n",
      "  BLEU Score:           0.0199\n",
      "  ROUGE-L:              0.1895\n",
      "  BERT F1:              0.8625\n",
      "  Vocabulary Overlap:   0.1949\n",
      "\n",
      "MEDICAL TERMINOLOGY:\n",
      "  Keyword Precision:    0.4927\n",
      "  Keyword Recall:       0.1673\n",
      "\n",
      "CAPTION STATISTICS:\n",
      "  Avg Reference Length: 33.4 words\n",
      "  Avg Prediction Length:8.4 words\n",
      "  Valid Predictions:    567/647\n",
      "  Empty Predictions:    80\n",
      "============================================================\n",
      "\n",
      "SAMPLE PREDICTIONS:\n",
      "----------------------------------------\n",
      "Reference 1: The lungs are clear without focal consolidation, effusion, or pneumothorax. Normal heart size. Bony ...\n",
      "Prediction 1: ...\n",
      "----------------------------------------\n",
      "Reference 2: Heart size within normal limits. Prominent right perihilar density consistent with lymphadenopathy, ...\n",
      "Prediction 2: X unchanged. No focal alveolar consolidation, no definite pleural effusion seen....\n",
      "----------------------------------------\n",
      "Reference 3: The heart is borderline size. Aorta is atherosclerotic. The mediastinum is stable. The lungs are cle...\n",
      "Prediction 3: ...\n",
      "----------------------------------------\n",
      "Reference 4: Heart size and mediastinal contour normal. Lungs are clear except for residuals of prior granulomato...\n",
      "Prediction 4:  or pneumothoraces. Visualized osseous structures unremarkable....\n",
      "----------------------------------------\n",
      "Reference 5: Lungs are clear without focal consolidation, effusion, or pneumothorax. Normal heart size. Scattered...\n",
      "Prediction 5:  degenerative changes of the thoracic spine. Bony thorax and soft tissues grossly...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Better evaluation metrics for medical captioning\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "def evaluate_medical_captions(references, predictions):\n",
    "    \"\"\"Enhanced evaluation for medical image captioning with better error handling\"\"\"\n",
    "    \n",
    "    # Filter out empty predictions for better analysis\n",
    "    valid_pairs = [(ref, pred) for ref, pred in zip(references, predictions) \n",
    "                   if pred.strip() != \"\" and ref.strip() != \"\"]\n",
    "    \n",
    "    if len(valid_pairs) == 0:\n",
    "        print(\"ERROR: No valid prediction-reference pairs found!\")\n",
    "        return {'error': 'no_valid_pairs'}\n",
    "    \n",
    "    valid_refs, valid_preds = zip(*valid_pairs)\n",
    "    \n",
    "    print(f\"Evaluating {len(valid_pairs)} valid pairs out of {len(references)} total\")\n",
    "    \n",
    "    # 1. BLEU Score with better tokenization\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    import re\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for ref, pred in zip(valid_refs, valid_preds):\n",
    "        # Better tokenization for medical text\n",
    "        ref_tokens = re.findall(r'\\b\\w+\\b', ref.lower())\n",
    "        pred_tokens = re.findall(r'\\b\\w+\\b', pred.lower())\n",
    "        \n",
    "        if len(pred_tokens) > 0:  # Avoid empty predictions\n",
    "            score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smooth_fn)\n",
    "            bleu_scores.append(score)\n",
    "    \n",
    "    # 2. ROUGE-L Score\n",
    "    from rouge_score import rouge_scorer\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for ref, pred in zip(valid_refs, valid_preds):\n",
    "        if pred.strip():  # Only if prediction is not empty\n",
    "            score = rouge_scorer_obj.score(ref, pred)['rougeL'].fmeasure\n",
    "            rouge_scores.append(score)\n",
    "    \n",
    "    # 3. BERTScore (semantic similarity) - only on valid pairs\n",
    "    bert_f1 = 0.0\n",
    "    if len(valid_preds) > 0:\n",
    "        try:\n",
    "            from bert_score import score as bert_score\n",
    "            # Filter out empty predictions for BERTScore\n",
    "            non_empty_preds = [p for p in valid_preds if p.strip()]\n",
    "            non_empty_refs = [valid_refs[i] for i, p in enumerate(valid_preds) if p.strip()]\n",
    "            \n",
    "            if len(non_empty_preds) > 0:\n",
    "                P, R, F1 = bert_score(non_empty_preds, non_empty_refs, lang=\"en\", verbose=False)\n",
    "                bert_f1 = F1.mean().item()\n",
    "        except Exception as e:\n",
    "            print(f\"BERTScore error: {e}\")\n",
    "            bert_f1 = 0.0\n",
    "    \n",
    "    # 4. Enhanced Medical keyword overlap\n",
    "    medical_keywords = [\n",
    "        # Basic anatomy\n",
    "        'heart', 'lungs', 'chest', 'ribs', 'diaphragm', 'mediastinum',\n",
    "        # Positions/orientations\n",
    "        'bilateral', 'right', 'left', 'upper', 'lower', 'middle', 'base', 'apex',\n",
    "        'posterior', 'anterior', 'lateral', 'medial',\n",
    "        # Normal findings\n",
    "        'normal', 'clear', 'unremarkable', 'stable', 'unchanged',\n",
    "        # Abnormal findings\n",
    "        'abnormal', 'pneumonia', 'consolidation', 'effusion', 'infiltrate',\n",
    "        'cardiomegaly', 'atelectasis', 'pneumothorax', 'opacity', 'nodule',\n",
    "        'mass', 'lesion', 'edema', 'congestion', 'hyperinflation',\n",
    "        # Descriptors\n",
    "        'increased', 'decreased', 'enlarged', 'small', 'large', 'prominent',\n",
    "        'mild', 'moderate', 'severe', 'acute', 'chronic'\n",
    "    ]\n",
    "    \n",
    "    keyword_overlap_scores = []\n",
    "    medical_recall_scores = []\n",
    "    \n",
    "    for ref, pred in zip(valid_refs, valid_preds):\n",
    "        ref_words = set(re.findall(r'\\b\\w+\\b', ref.lower()))\n",
    "        pred_words = set(re.findall(r'\\b\\w+\\b', pred.lower()))\n",
    "        \n",
    "        ref_keywords = ref_words.intersection(set(medical_keywords))\n",
    "        pred_keywords = pred_words.intersection(set(medical_keywords))\n",
    "        \n",
    "        # Precision: how many predicted keywords are correct\n",
    "        if len(pred_keywords) == 0:\n",
    "            precision = 1.0 if len(ref_keywords) == 0 else 0.0\n",
    "        else:\n",
    "            precision = len(ref_keywords.intersection(pred_keywords)) / len(pred_keywords)\n",
    "        \n",
    "        # Recall: how many reference keywords were predicted\n",
    "        if len(ref_keywords) == 0:\n",
    "            recall = 1.0 if len(pred_keywords) == 0 else 0.0\n",
    "        else:\n",
    "            recall = len(ref_keywords.intersection(pred_keywords)) / len(ref_keywords)\n",
    "        \n",
    "        keyword_overlap_scores.append(precision)\n",
    "        medical_recall_scores.append(recall)\n",
    "    \n",
    "    # 5. Caption length analysis\n",
    "    avg_ref_length = sum(len(ref.split()) for ref in valid_refs) / len(valid_refs)\n",
    "    avg_pred_length = sum(len(pred.split()) for pred in valid_preds) / len(valid_preds)\n",
    "    \n",
    "    # 6. Vocabulary overlap\n",
    "    all_ref_words = set()\n",
    "    all_pred_words = set()\n",
    "    for ref, pred in zip(valid_refs, valid_preds):\n",
    "        all_ref_words.update(ref.lower().split())\n",
    "        all_pred_words.update(pred.lower().split())\n",
    "    \n",
    "    vocab_overlap = len(all_ref_words.intersection(all_pred_words)) / len(all_ref_words.union(all_pred_words))\n",
    "    \n",
    "    return {\n",
    "        'bleu': sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0,\n",
    "        'rouge_l': sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0.0,\n",
    "        'bert_f1': bert_f1,\n",
    "        'medical_keyword_precision': sum(keyword_overlap_scores) / len(keyword_overlap_scores) if keyword_overlap_scores else 0.0,\n",
    "        'medical_keyword_recall': sum(medical_recall_scores) / len(medical_recall_scores) if medical_recall_scores else 0.0,\n",
    "        'vocab_overlap': vocab_overlap,\n",
    "        'avg_ref_length': avg_ref_length,\n",
    "        'avg_pred_length': avg_pred_length,\n",
    "        'valid_predictions': len(valid_pairs),\n",
    "        'total_samples': len(references),\n",
    "        'empty_predictions': len(references) - len(valid_pairs)\n",
    "    }\n",
    "\n",
    "# Run improved evaluation\n",
    "references = [ref['caption'] for ref in evalRefs]\n",
    "predictions = [hyp['caption'] for hyp in evalHyps]\n",
    "\n",
    "print(\"Computing enhanced evaluation metrics...\")\n",
    "results = evaluate_medical_captions(references, predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'error' not in results:\n",
    "    print(\"SIMILARITY METRICS:\")\n",
    "    print(f\"  BLEU Score:           {results['bleu']:.4f}\")\n",
    "    print(f\"  ROUGE-L:              {results['rouge_l']:.4f}\")\n",
    "    print(f\"  BERT F1:              {results['bert_f1']:.4f}\")\n",
    "    print(f\"  Vocabulary Overlap:   {results['vocab_overlap']:.4f}\")\n",
    "    \n",
    "    print(\"\\nMEDICAL TERMINOLOGY:\")\n",
    "    print(f\"  Keyword Precision:    {results['medical_keyword_precision']:.4f}\")\n",
    "    print(f\"  Keyword Recall:       {results['medical_keyword_recall']:.4f}\")\n",
    "    \n",
    "    print(\"\\nCAPTION STATISTICS:\")\n",
    "    print(f\"  Avg Reference Length: {results['avg_ref_length']:.1f} words\")\n",
    "    print(f\"  Avg Prediction Length:{results['avg_pred_length']:.1f} words\")\n",
    "    print(f\"  Valid Predictions:    {results['valid_predictions']}/{results['total_samples']}\")\n",
    "    print(f\"  Empty Predictions:    {results['empty_predictions']}\")\n",
    "else:\n",
    "    print(\"ERROR in evaluation:\", results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show some example predictions for debugging\n",
    "print(\"\\nSAMPLE PREDICTIONS:\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(min(5, len(references))):\n",
    "    print(f\"Reference {i+1}: {references[i][:100]}...\")\n",
    "    print(f\"Prediction {i+1}: {predictions[i][:100]}...\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 516716,
     "sourceId": 951996,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "xray-captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
